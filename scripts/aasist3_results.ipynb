{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "955cf0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bcd52c",
   "metadata": {},
   "source": [
    "# AASIST-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d7efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lx/klbf168d1715rmbbfm7mswhh0000gp/T/ipykernel_25721/979416473.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  aasist3_test['label'] = aasist3_test['label'].replace({'fake': 0, 'real': 1})\n"
     ]
    }
   ],
   "source": [
    "aasist3_test = pd.read_csv(\"../results/AASIST3/AASIST3_AFAD_test_scores.txt\", sep=\"\\t\")\n",
    "aasist3_test.columns=[\"speaker_id\", \"audio\", \"file_name\", \"label\", \"gender\", \"tts\", \"x\", \"predicted\", \"xx\"]\n",
    "aasist3_test['label'] = aasist3_test['label'].replace({'fake': 0, 'real': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c264de",
   "metadata": {},
   "outputs": [],
   "source": [
    "aasist3_eleven = aasist3_test.query(\"tts in ['eleven_multilingual_v2', 'none'] and label in [1, 0]\")\n",
    "aasist3_openai = aasist3_test.query(\"tts in ['gpt-4o-mini-tts', 'none'] and label in [1, 0]\")\n",
    "aasist3_minimax = aasist3_test.query(\"tts in ['speech-2.5-hd-preview', 'none'] and label in [1, 0]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9d7a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, accuracy, F1 micro and macro scores using sklearn.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Ground truth labels\n",
    "    y_pred : array-like\n",
    "        Predicted labels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing all metrics\n",
    "    \"\"\"\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Compute precision (micro and macro)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    # Compute recall (micro and macro)\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    # Compute F1 (micro and macro)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    # Compute per-class metrics\n",
    "    precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    \n",
    "    # Get unique classes\n",
    "    classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision_micro': precision_micro,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_micro': recall_micro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'per_class': {\n",
    "            'precision': dict(zip(classes, precision_per_class)),\n",
    "            'recall': dict(zip(classes, recall_per_class)),\n",
    "            'f1': dict(zip(classes, f1_per_class))\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def save_metrics_to_file(metrics, filename='metrics_results.txt'):\n",
    "    \"\"\"\n",
    "    Save computed metrics to a text file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    metrics : dict\n",
    "        Dictionary containing metrics from compute_metrics function\n",
    "    filename : str\n",
    "        Name of the output file (default: 'metrics_results.txt')\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        f.write(\"CLASSIFICATION METRICS RESULTS\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        # Write overall metrics\n",
    "        f.write(\"Overall Metrics:\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        f.write(f\"Accuracy: {metrics['accuracy']:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Precision (Micro): {metrics['precision_micro']:.4f}\\n\")\n",
    "        f.write(f\"Precision (Macro): {metrics['precision_macro']:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Recall (Micro): {metrics['recall_micro']:.4f}\\n\")\n",
    "        f.write(f\"Recall (Macro): {metrics['recall_macro']:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"F1 Score (Micro): {metrics['f1_micro']:.4f}\\n\")\n",
    "        f.write(f\"F1 Score (Macro): {metrics['f1_macro']:.4f}\\n\\n\")\n",
    "        \n",
    "        # Write per-class metrics\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        f.write(\"Per-Class Metrics:\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        for class_label in sorted(metrics['per_class']['precision'].keys()):\n",
    "            f.write(f\"Class {class_label}:\\n\")\n",
    "            f.write(f\"  Precision: {metrics['per_class']['precision'][class_label]:.4f}\\n\")\n",
    "            f.write(f\"  Recall:    {metrics['per_class']['recall'][class_label]:.4f}\\n\")\n",
    "            f.write(f\"  F1 Score:  {metrics['per_class']['f1'][class_label]:.4f}\\n\\n\")\n",
    "    \n",
    "    print(f\"Metrics saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b639cd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to ../results/AASIST3/AASIST3_AFAD_test_results.txt\n",
      "Metrics saved to ../results/AASIST3/AASIST3_eleven_results.txt\n",
      "Metrics saved to ../results/AASIST3/AASIST3_openai_results.txt\n",
      "Metrics saved to ../results/AASIST3/AASIST3_minimax_results.txt\n"
     ]
    }
   ],
   "source": [
    "save_metrics_to_file(compute_metrics(aasist3_test['label'].to_list(), aasist3_test['predicted'].to_list()), '../results/AASIST3/AASIST3_AFAD_test_results.txt')\n",
    "save_metrics_to_file(compute_metrics(aasist3_eleven['label'].to_list(), aasist3_eleven['predicted'].to_list()), '../results/AASIST3/AASIST3_eleven_results.txt')\n",
    "save_metrics_to_file(compute_metrics(aasist3_openai['label'].to_list(), aasist3_openai['predicted'].to_list()), '../results/AASIST3/AASIST3_openai_results.txt')\n",
    "save_metrics_to_file(compute_metrics(aasist3_minimax['label'].to_list(), aasist3_minimax['predicted'].to_list()), '../results/AASIST3/AASIST3_minimax_results.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd0d4f",
   "metadata": {},
   "source": [
    "# Ad-hoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ee0f2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to ../results/AASIST3/AASIST3_fish_test_results.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lx/klbf168d1715rmbbfm7mswhh0000gp/T/ipykernel_25721/662290558.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  fish['label'] = fish['label'].replace({'fake': 0, 'real': 1})\n"
     ]
    }
   ],
   "source": [
    "fish = pd.read_csv(\"../results/AASIST3/AASIST3_fish_scores.txt\", sep=\"\\t\")\n",
    "fish.columns=[\"speaker_id\", \"audio\", \"file_name\", \"label\", \"gender\", \"tts\", \"x\", \"predicted\", \"xx\"]\n",
    "fish['label'] = fish['label'].replace({'fake': 0, 'real': 1})\n",
    "save_metrics_to_file(compute_metrics(fish['label'].to_list(), fish['predicted'].to_list()), '../results/AASIST3/AASIST3_fish_test_results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bb9b3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to ../results/AASIST3/AASIST3_xtts_test_results.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lx/klbf168d1715rmbbfm7mswhh0000gp/T/ipykernel_25721/693726111.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  xtts['label'] = xtts['label'].replace({'fake': 0, 'real': 1})\n"
     ]
    }
   ],
   "source": [
    "xtts = pd.read_csv(\"../results/AASIST3/AASIST3_xtts_scores.txt\", sep=\"\\t\")\n",
    "xtts.columns=[\"speaker_id\", \"audio\", \"file_name\", \"label\", \"gender\", \"tts\", \"x\", \"predicted\", \"xx\"]\n",
    "xtts['label'] = xtts['label'].replace({'fake': 0, 'real': 1})\n",
    "save_metrics_to_file(compute_metrics(xtts['label'].to_list(), xtts['predicted'].to_list()), '../results/AASIST3/AASIST3_xtts_test_results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "536e1275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to ../results/AASIST3/AASIST3_mms_test_results.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lx/klbf168d1715rmbbfm7mswhh0000gp/T/ipykernel_25721/192686005.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  mms['label'] = mms['label'].replace({'fake': 0, 'real': 1})\n"
     ]
    }
   ],
   "source": [
    "mms = pd.read_csv(\"../results/AASIST3/AASIST3_mms_scores.txt\", sep=\"\\t\")\n",
    "mms.columns=[\"speaker_id\", \"audio\", \"file_name\", \"label\", \"gender\", \"tts\", \"x\", \"predicted\", \"xx\"]\n",
    "mms['label'] = mms['label'].replace({'fake': 0, 'real': 1})\n",
    "save_metrics_to_file(compute_metrics(mms['label'].to_list(), mms['predicted'].to_list()), '../results/AASIST3/AASIST3_mms_test_results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46d5f021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to ../results/AASIST3/AASIST3_T5_test_results.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lx/klbf168d1715rmbbfm7mswhh0000gp/T/ipykernel_25721/2378442507.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  t5['label'] = t5['label'].replace({'fake': 0, 'real': 1})\n"
     ]
    }
   ],
   "source": [
    "t5 = pd.read_csv(\"../results/AASIST3/AASIST3_T5_scores.txt\", sep=\"\\t\")\n",
    "t5.columns=[\"speaker_id\", \"audio\", \"file_name\", \"label\", \"gender\", \"tts\", \"x\", \"predicted\", \"xx\"]\n",
    "t5['label'] = t5['label'].replace({'fake': 0, 'real': 1})\n",
    "save_metrics_to_file(compute_metrics(t5['label'].to_list(), t5['predicted'].to_list()), '../results/AASIST3/AASIST3_T5_test_results.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakeaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
